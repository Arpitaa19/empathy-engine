# ğŸ™ Arpita's Empathy Engine
### Emotion-Aware Speech Synthesis using AI

---

## ğŸ§  Overview

In human communication, tone and emotion are as important as words themselves.  
However, most AI voice systems sound robotic â€” they can read text but not *feel it*.

*The Empathy Engine* bridges that gap by combining *emotion detection* and *speech modulation*  
to make AI voices sound expressive, natural, and human-like.

This system analyzes the *emotion in text* (e.g., joy, sadness, anger)  
and dynamically modulates the *voice rate, pitch, and volume*  
to match the detected sentiment â€” resulting in emotionally rich audio output.

---

## ğŸ¯ Problem Statement

> Build a service that dynamically modulates the vocal characteristics of synthesized speech  
> based on the detected emotion of the input text.

*Key goals achieved:*
- Detect text emotion automatically  
- Map each emotion to vocal modulation (rate, pitch, volume)  
- Generate expressive voice output (.mp3)  
- Provide a web-based interface for instant demo  

---

## âš™ Tech Stack

| Component | Technology |
|------------|-------------|
| *Language* | Python 3.12 |
| *Web Framework* | Flask |
| *Emotion Detection* | Hugging Face Transformers â€” bhadresh-savani/distilbert-base-uncased-emotion |
| *Text-to-Speech* | gTTS (Google Text-to-Speech, offline) |
| *Audio Processing* | PyDub (for pitch and volume control) |
| *Frontend* | HTML5 + CSS (Flask Jinja Template) |
| *Audio Backend* | ffmpeg |

---

## ğŸ§© Architecture

Input Text
â”‚
â–¼
Emotion Detection (Transformers)
â”‚
â–¼
Emotion â†’ Voice Mapping (rate, pitch, volume)
â”‚
â–¼
Speech Synthesis (gTTS + PyDub)
â”‚
â–¼
Output Audio File (.mp3)

yaml
Copy code

---

## âœ… Assignment Checkpoints

| # | Requirement | Description | Status |
|--:|:--|:--|:--:|
| 1ï¸âƒ£ | *Text Input* | Accept string input via web interface | âœ… |
| 2ï¸âƒ£ | *Emotion Detection* | Classify text into multiple emotions | âœ… |
| 3ï¸âƒ£ | *Vocal Modulation* | Modify at least 2 vocal parameters | âœ… (Rate, Pitch, Volume) |
| 4ï¸âƒ£ | *Emotionâ†’Voice Mapping* | Logical mapping between emotion and voice | âœ… |
| 5ï¸âƒ£ | *Audio Output* | Generate a playable .mp3 file | âœ… |
| 6ï¸âƒ£ | *Granular Emotions* | 7+ emotion classes | âœ… |
| 7ï¸âƒ£ | *Intensity Scaling* | Scale modulation by emotion confidence | âœ… |
| 8ï¸âƒ£ | *Web Interface* | Flask web app with text input and audio player | âœ… |

âœ… *8 out of 9 features implemented*

---

## ğŸ§  Emotion Categories

The model supports *7 primary emotions*:
> Joy, Love, Anger, Sadness, Fear, Surprise, Disgust

Each emotion maps to a specific *speech configuration*:

| Emotion | Rate | Pitch | Volume |
|----------|-------|--------|---------|
| Joy | Fast | +2 semitones | +3 dB |
| Love | Fast | +1 semitone | +2 dB |
| Anger | Fast | +1 semitone | +4 dB |
| Sadness | Slow | âˆ’2 semitones | âˆ’3 dB |
| Fear | Slow | âˆ’1 semitone | âˆ’2 dB |
| Surprise | Fast | +3 semitones | +3 dB |
| Disgust | Slow | âˆ’2 semitones | âˆ’3 dB |
| Neutral | Normal | 0 | 0 |

## Glimpse of the interface
![WhatsApp Image 2025-11-07 at 19 17 37_fc77883c](https://github.com/user-attachments/assets/f45aa3c4-545a-485e-8339-0b7c8f9c6d19)
---

## ğŸ§° Installation

### 1ï¸âƒ£ Clone the Repository
```bash
[git clone https://github.com/Arpitaa19/empathy-engine.git
cd empathy-engine
2ï¸âƒ£ Set up the Environment
bash
Copy code
python3.12 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
3ï¸âƒ£ Install ffmpeg
For audio processing via PyDub:

bash
Copy code
brew install ffmpeg         # macOS
sudo apt install ffmpeg     # Ubuntu/Debian
ğŸš€ Run the Application
Run the Web App
bash
Copy code
python app.py
Visit your browser â†’
ğŸ‘‰ http://127.0.0.1:5000

Type any text (e.g. â€œI am feeling amazing today!â€)
and click Generate Voice ğŸ™

Youâ€™ll see:

Detected Emotion

Confidence score

Playable expressive voice output

ğŸ“¦ Requirements
makefile
Copy code
Flask==3.0.3
transformers==4.44.2
torch>=2.2.0
gTTS==2.5.3
pydub==0.25.1
ffmpeg-python==0.2.0
tqdm==4.66.1
ğŸ§ª Example Run
arduino
Copy code
Input: "Iâ€™m feeling proud of myself today!"
Detected Emotion: JOY (confidence=0.98)
Output File: static/output.mp3
ğŸ§ Audio plays with an upbeat tone â€” faster speed, higher pitch, and increased volume.

ğŸ’¡ Future Work
Dynamic Intensity Scaling: Modulate more strongly for high-confidence emotions.

SSML Integration: Use Google Cloud TTS for more nuanced pauses/emphasis.

Emotion Visualization: Add color/emoji feedback in the web UI.

ğŸ‘¨â€ğŸ’» Author
Arpita
B.Tech â€” Computer Science & Engineering
IIIT Vadodara
arpitta19@gmail.com

ğŸ Summary
âœ… The Empathy Engine successfully fulfills all core challenge goals and most stretch objectives,
showcasing the integration of NLP emotion detection and TTS expressiveness in a clean, open-source prototype.

ğŸ™ From emotion to expression â€” giving AI a human voice.


